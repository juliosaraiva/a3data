# ===========================================
# APPLICATION CONFIGURATION
# ===========================================
APP_NAME="Incident Extractor API"
APP_VERSION="1.0.0"
DEBUG=false
ENVIRONMENT="development"

# ===========================================
# LLM CONFIGURATION
# ===========================================
# Provider: ollama, openai, gemini, perplexity
LLM_PROVIDER="ollama"

# API Key (required for cloud providers: openai, gemini, perplexity)
LLM_API_KEY=""

# Model name (optional - uses provider default if not specified)
# Ollama: llama3.2:3b, llama2:7b, codellama:7b
# OpenAI: gpt-4o-mini, gpt-4o, gpt-3.5-turbo
# Gemini: gemini-1.5-flash, gemini-1.5-pro
# Perplexity: llama-3.1-sonar-small-128k-online
LLM_MODEL_NAME="gemma3:4b"

# Generation parameters
LLM_TEMPERATURE=0.1  # Low temperature for consistent extraction
LLM_MAX_TOKENS=1000
LLM_TIMEOUT=30

# Base URL (mainly for Ollama)
LLM_BASE_URL="http://localhost:11434"

# ===========================================
# LOGGING CONFIGURATION
# ===========================================
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL="INFO"
LOGGING_LEVEL="INFO"  # Alternative alias

# Logging format: json, text
LOG_FORMAT="json"

# Console and file output
LOG_CONSOLE_ENABLED="true"
LOG_FILE_ENABLED="true"
LOG_FILE_PATH="logs/app.log"
LOG_MAX_FILE_SIZE=10485760  # 10MB
LOG_BACKUP_COUNT=5
LOG_CORRELATION_ID_ENABLED="true"

# ===========================================
# API CONFIGURATION
# ===========================================
DEFAULT_RETRY_ATTEMPTS=3
DEFAULT_RETRY_DELAY=5
DEFAULT_TIMEOUT=30
